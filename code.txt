TrÃ¨s bonne question.
ğŸ‘‰ Oui, câ€™est totalement possible de tout faire en .ipynb, et honnÃªtement câ€™est mÃªme mieux pour toi dans ta situation.

âœ… Est-ce mieux de faire les 3 Ã©tapes en .ipynb ?
âœ”ï¸ Avantages du format notebook (ipynb)

Visualisation directe des DataFrame

PossibilitÃ© dâ€™inspecter les landmarks

VÃ©rification immÃ©diate des sÃ©quences

Plus simple pour dÃ©bugger

Tu peux exÃ©cuter seulement certaines cellules

TrÃ¨s adaptÃ© pour lâ€™exploration de donnÃ©es dâ€™exercices

âœ”ï¸ Câ€™est exactement ce que lâ€™auteur du repo fait

Tu as vu :

1.data.ipynb

2.sklearn.ipynb

3.deep_learning.ipynb

etc.

Donc oui, nous allons faire pareil.

ğŸ“š ğŸ“ Architecture recommandÃ©e pour Plank

Dans ton projet, crÃ©e un dossier :

plank_model/
    1.data.ipynb
    2.sequences.ipynb
    3.train_lstm.ipynb
    model/
        input_scaler.pkl
        plank_model.keras   (ou .pth si PyTorch)


Exactement comme lâ€™auteur.

ğŸš€ Je te fournis maintenant les 3 notebooks complets

Tu nâ€™auras quâ€™Ã  copier les cellules dans :

ğŸ”¹ plank_model/1.data.ipynb
ğŸ”¹ plank_model/2.sequences.ipynb
ğŸ”¹ plank_model/3.train_lstm.ipynb
----------------------------------------------------
ğŸ“˜ Notebook 1 â€” 1.data.ipynb
----------------------------------------------------
â¡ï¸ Objectif : Rassembler toutes les donnÃ©es CSV Plank dans un dataset unique

Voici le contenu Ã  mettre dans ton notebook :

ğŸ”µ Cellule 1 â€” Imports
import pandas as pd
import numpy as np
import glob
import os

print("Imports ok")

ğŸ”µ Cellule 2 â€” DÃ©finir les dossiers contenant les CSV Plank
folders = [
    "notebooks/data/landmarks/raw_data/data-btc/plank",
    "notebooks/data/landmarks/raw_data/data-crawl/plank",
    "notebooks/data/landmarks/test/plank",
    "notebooks/data/landmarks/verified_data/data_btc_10s/plank",
    "notebooks/data/landmarks/verified_data/data_crawl_10s/plank"
]

folders

ğŸ”µ Cellule 3 â€” Charger tous les CSV
all_dfs = []

for folder in folders:
    csv_files = glob.glob(os.path.join(folder, "*.csv"))
    print(f"{folder}: {len(csv_files)} fichiers trouvÃ©s")

    for f in csv_files:
        df = pd.read_csv(f)
        df["label"] = "plank"
        all_dfs.append(df)

print("Total CSV chargÃ©s :", len(all_dfs))

ğŸ”µ Cellule 4 â€” ConcatÃ©ner le dataset
plank_dataset = pd.concat(all_dfs, ignore_index=True)
plank_dataset.head()

ğŸ”µ Cellule 5 â€” Sauvegarder
os.makedirs("plank_model/data", exist_ok=True)
plank_dataset.to_csv("plank_model/data/plank_dataset.csv", index=False)

print("Dataset sauvegardÃ© â†’ plank_model/data/plank_dataset.csv")
print("Shape :", plank_dataset.shape)

----------------------------------------------------
ğŸ“˜ Notebook 2 â€” 2.sequences.ipynb
----------------------------------------------------
â¡ï¸ Objectif : Transformer les points clÃ©s en sÃ©quences pour LSTM
ğŸ”µ Cellule 1 â€” Imports
import pandas as pd
import numpy as np
import os

print("Imports ok")

ğŸ”µ Cellule 2 â€” Charger le dataset
df = pd.read_csv("plank_model/data/plank_dataset.csv")
df.head()

ğŸ”µ Cellule 3 â€” Liste des colonnes landmarks
lm_cols = [c for c in df.columns if c not in ["label"]]
len(lm_cols), lm_cols[:10]

ğŸ”µ Cellule 4 â€” ParamÃ¨tres des sÃ©quences
SEQ_LEN = 50
STEP = 10

ğŸ”µ Cellule 5 â€” CrÃ©ation des sÃ©quences
X, y = [], []

for i in range(0, len(df) - SEQ_LEN, STEP):
    seq = df[lm_cols].iloc[i:i+SEQ_LEN].values
    X.append(seq)
    y.append(0)  # une seule classe pour l'instant

X = np.array(X)
y = np.array(y)

X.shape, y.shape

ğŸ”µ Cellule 6 â€” Sauvegarde
np.savez("plank_model/data/plank_sequences.npz", X=X, y=y)

print("SÃ©quences sauvegardÃ©es dans plank_model/data/plank_sequences.npz")

----------------------------------------------------
ğŸ“˜ Notebook 3 â€” 3.train_lstm.ipynb
----------------------------------------------------
â¡ï¸ Objectif : EntraÃ®ner un modÃ¨le LSTM pour reconnaÃ®tre la Plank
ğŸ”µ Cellule 1 â€” Imports
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
import os

print("Imports ok")

ğŸ”µ Cellule 2 â€” Charger les donnÃ©es
data = np.load("plank_model/data/plank_sequences.npz")
X = data["X"]
y = data["y"]

X.shape, y.shape

ğŸ”µ Cellule 3 â€” Construire le modÃ¨le LSTM
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    Dropout(0.3),
    LSTM(64),
    Dropout(0.3),
    Dense(32, activation="relu"),
    Dense(1, activation="sigmoid")
])

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.summary()

ğŸ”µ Cellule 4 â€” EntraÃ®nement
os.makedirs("plank_model/model", exist_ok=True)

checkpoint = ModelCheckpoint(
    "plank_model/model/plank_model.keras",
    save_best_only=True,
    monitor="val_loss"
)

history = model.fit(
    X, y,
    validation_split=0.2,
    epochs=20,
    batch_size=32,
    callbacks=[checkpoint]
)

ğŸ”µ Cellule 5 â€” Sauvegarde finale
model.save("plank_model/model/plank_model_final.keras")
print("ModÃ¨le sauvegardÃ©.")

ğŸ‰ Et voilÃ  !

Tu as :

âœ”ï¸ Le notebook pour charger les CSV
âœ”ï¸ Le notebook pour crÃ©er les sÃ©quences
âœ”ï¸ Le notebook pour entraÃ®ner le modÃ¨le
âœ”ï¸ Une architecture propre comme lâ€™auteur du repo
âœ”ï¸ Un modÃ¨le fonctionnel pour Plank

ğŸ‘‰ Prochaine Ã©tape

Veux-tu que je te gÃ©nÃ¨re :

ğŸ”¹ Le systÃ¨me de dÃ©tection en temps rÃ©el pour Plank (comme lâ€™autre repo) ?
ğŸ”¹ Le pipeline dâ€™infÃ©rence (plank.py) ?
ğŸ”¹ Ou on passe Ã  lâ€™exercice suivant : Squad ou Push-Up ?